{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "img=cv2.imread('lena.jpg',1)\n",
    "cv2.imshow('image',img)\n",
    "print(img)\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()\n",
    "elif k==ord('n'):\n",
    "    cv2.imwrite('lena_copy.png',img)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to capture live video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap=cv2.VideoCapture(0);\n",
    "fourcc=cv2.VideoWriter_fourcc(*'XVID')\n",
    "out=cv2.VideoWriter('output.avi',fourcc,20.0, (640,480))\n",
    "\n",
    "\n",
    "\n",
    "print(cap.isOpened())\n",
    "while(cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "    if ret==True:\n",
    "        print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "        print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        cv2.imshow('video',gray)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert line,circle,rectangle on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "img=cv2.line(img,(0,0),(512,512),(0,0,255),2)\n",
    "img=cv2.arrowedLine(img,(0,512),(255,0),(0,255,0),10)\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "img=cv2.line(img,(0,0),(512,512),(0,0,255),2)\n",
    "img=cv2.arrowedLine(img,(0,512),(500,0),(0,255,0),10)\n",
    "\n",
    "img=cv2.rectangle(img,(194,52),(313,154),(0,255,0),5,-1)\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "\n",
    "k = cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "img=cv2.line(img,(0,0),(512,512),(0,0,255),2)\n",
    "img=cv2.arrowedLine(img,(0,512),(500,0),(0,255,0),10)\n",
    "\n",
    "img=cv2.rectangle(img,(194,52),(313,154),(0,0,255),-1)\n",
    "\n",
    "img=cv2.circle(img,(240,440),65,(0,255,0),-1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "\n",
    "k = cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "font=cv2.FONT_ITALIC\n",
    "\n",
    "cv2.putText(img,'Learning OpenCV',(150,200),font,2,(255,255,255),5,cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "\n",
    "k = cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to put text on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#img=cv2.imread('lena.jpg',1)\n",
    "img=np.zeros([512,512,3],np.uint8)\n",
    "\n",
    "font=cv2.FONT_ITALIC\n",
    "\n",
    "img=cv2.rectangle(img,(194,52),(313,154),(0,0,255),-1)\n",
    "\n",
    "img=cv2.circle(img,(240,440),65,(0,255,0),-1)\n",
    "\n",
    "cv2.putText(img,'Learning OpenCV',(10,200),font,2,(0,255,255),3,cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "\n",
    "k = cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "cap.set(3,1208)\n",
    "cap.set(4,720)\n",
    "\n",
    "print(cap.get(3))\n",
    "print(cap.get(4))\n",
    "\n",
    "print(cap.isOpened())\n",
    "while(cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "    if ret==True:\n",
    "            \n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        cv2.imshow('video',gray)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "cap.set(3,1208)\n",
    "cap.set(4,720)\n",
    "\n",
    "print(cap.get(3))\n",
    "print(cap.get(4))\n",
    "\n",
    "print(cap.isOpened())\n",
    "while(cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "    if ret==True:\n",
    "            \n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        cv2.imshow('video',gray)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "#cap.set(3,640)\n",
    "#cap.set(4,480)\n",
    "\n",
    "#print(cap.get(3))\n",
    "#print(cap.get(4))\n",
    "\n",
    "print(cap.isOpened())\n",
    "while(cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "    if ret==True:\n",
    "            \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX \n",
    "        \n",
    "        text='Width:'+str(cap.get(3))+ ' Height:'+str(cap.get(4))\n",
    "       \n",
    "        datet=str(datetime.datetime.now())\n",
    "        \n",
    "        frame=cv2.putText(frame,datet,(10,30),font,1,(0,255,255),1,cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('video',frame)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "events=[ i for i in dir(cv2) if 'EVENT' in i]\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def click_event(event,x,y,flags,param):\n",
    "\n",
    "    if event==cv2.EVENT_LBUTTONDOWN:\n",
    "        print(x,',  ',y)\n",
    "       \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "              \n",
    "        strXY=str(x) +',  '+ str(y)\n",
    "            \n",
    "        cv2.putText(img,strXY,(x,y),font,1,(255,255,0),2)\n",
    "            \n",
    "        cv2.imshow('image',img)\n",
    "            \n",
    "img=np.zeros([512,512,3],np.uint8)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def click_event(event,x,y,flags,param):\n",
    "\n",
    "    if event==cv2.EVENT_LBUTTONDOWN:\n",
    "        print(x,',  ',y)\n",
    "       \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "              \n",
    "        strXY=str(x) +',  '+ str(y)\n",
    "            \n",
    "        cv2.putText(img,strXY,(x,y),font,0.5,(255,255,0),2)\n",
    "                        \n",
    "        cv2.imshow('image',img)\n",
    "        \n",
    "    if event==cv2.EVENT_RBUTTONDOWN:\n",
    "        \n",
    "        blue=img[y,x,0]\n",
    "        green=img[y,x,1]\n",
    "        red=img[y,x,2]\n",
    "        \n",
    "               \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "        strBGR=str(blue) +',  '+ str(green) +',  '+str(red)\n",
    "            \n",
    "        cv2.putText(img,strBGR,(x,y),font,0.5,(0,255,255),2)\n",
    "                        \n",
    "        cv2.imshow('image',img)\n",
    "        \n",
    "        \n",
    "            \n",
    "img=np.zeros([512,512,3],np.uint8)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def click_event(event,x,y,flags,param):\n",
    "\n",
    "    if event==cv2.EVENT_LBUTTONDOWN:\n",
    "        print(x,',  ',y)\n",
    "       \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "              \n",
    "        strXY=str(x) +',  '+ str(y)\n",
    "            \n",
    "        cv2.putText(img,strXY,(x,y),font,0.5,(255,255,0),2)\n",
    "                        \n",
    "        cv2.imshow('image',img)\n",
    "        \n",
    "    if event==cv2.EVENT_RBUTTONDOWN:\n",
    "        \n",
    "        blue=img[y,x,0]\n",
    "        green=img[y,x,1]\n",
    "        red=img[y,x,2]\n",
    "        \n",
    "               \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "        strBGR=str(blue) +',  '+ str(green) +',  '+str(red)\n",
    "            \n",
    "        cv2.putText(img,strBGR,(x,y),font,0.5,(0,255,255),2)\n",
    "                        \n",
    "        cv2.imshow('image',img)\n",
    "        \n",
    "        \n",
    "            \n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def click_event(event,x,y,flags,param):\n",
    "\n",
    "    if event==cv2.EVENT_LBUTTONDOWN:\n",
    "        print(x,',  ',y)\n",
    "       \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "              \n",
    "        strXY=str(x) +',  '+ str(y)\n",
    "            \n",
    "        cv2.putText(img,strXY,(x,y),font,0.5,(255,255,0),2)\n",
    "                        \n",
    "        cv2.imshow('image',img)\n",
    "        \n",
    "    if event==cv2.EVENT_RBUTTONDOWN:\n",
    "        \n",
    "        blue=img[y,x,0]\n",
    "        green=img[y,x,1]\n",
    "        red=img[y,x,2]\n",
    "        \n",
    "               \n",
    "        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "        strBGR=str(blue) +',  '+ str(green) +',  '+str(red)\n",
    "            \n",
    "        cv2.putText(img,strBGR,(x,y),font,0.5,(0,255,255),2)\n",
    "                        \n",
    "        cv2.imshow('image',img)\n",
    "        \n",
    "        \n",
    "            \n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def click_event(event,x,y,flags,param):\n",
    "\n",
    "    if event==cv2.EVENT_LBUTTONDOWN:\n",
    "        \n",
    "        cv2.circle(img,(x,y),3,(0,0,255),-1,)\n",
    "        \n",
    "        points.append((x,y))\n",
    "        if len(points)>=2:\n",
    "            cv2.line(img,points[-1],points[-2],(255,0,0),5 )\n",
    "        \n",
    "        cv2.imshow('image',img)\n",
    "   \n",
    "        \n",
    "        \n",
    "img=np.zeros((512,512,3),np.uint8)           \n",
    "\n",
    "#img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "points=[]\n",
    "\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def click_event(event,x,y,flags,param):\n",
    "\n",
    "    if event==cv2.EVENT_LBUTTONDOWN:\n",
    "        \n",
    "        blue=img[x,y,0]\n",
    "        green=img[x,y,1]\n",
    "        red=img[x,y,2]\n",
    "        cv2.circle(img,(x,y),3,(0,0,255),-1)\n",
    "        mycolorimage=np.zeros((512,512,3),np.uint8)\n",
    "        \n",
    "        mycolorimage[:]=[blue,green,red]\n",
    "        \n",
    "        cv2.imshow('color',mycolorimage)\n",
    "   \n",
    "        \n",
    "        \n",
    "#img=np.zeros((512,512,3),np.uint8)           \n",
    "\n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "points=[]\n",
    "\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread('messi5.jpg')\n",
    "print(img.shape)\n",
    "print(img.size)\n",
    "print(img.dtype)\n",
    "\n",
    "b,g,r=cv2.split(img)\n",
    "img=cv2.merge((b,g,r))\n",
    "\n",
    "ball=img[280:340, 330:390]\n",
    "\n",
    "img[273:333, 100:160]=ball\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread('messi5.jpg')\n",
    "\n",
    "img2=cv2.imread('opencv-logo.png') \n",
    "\n",
    "print(img.shape)\n",
    "print(img.size)\n",
    "print(img.dtype)\n",
    "\n",
    "b,g,r=cv2.split(img)\n",
    "img=cv2.merge((b,g,r))\n",
    "\n",
    "ball=img[280:340, 330:390]\n",
    "\n",
    "img[273:333, 100:160]=ball\n",
    "\n",
    "img=cv2.resize(img,(512,512))\n",
    "\n",
    "img2=cv2.resize(img2,(512,512))\n",
    "\n",
    "#dst=cv2.add(img,img2)\n",
    "\n",
    "dst=cv2.addWeighted(img,.9,img2,.2,0);\n",
    "\n",
    "cv2.imshow('image',dst)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1=np.zeros((512,512,3),np.uint8)\n",
    "img1=cv2.rectangle(img1,(200,0),(300,100),(255,255,255),-1)\n",
    "\n",
    "img2=cv2.imread('lena.jpg')\n",
    "\n",
    "#img2=cv2.resize(img2,(250,500))\n",
    "\n",
    "#bitAnd=cv2.bitwise_and(img1,img2)\n",
    "\n",
    "#bitxor=cv2.bitwise_xor(img1,img2)\n",
    "\n",
    "bitnot1=cv2.bitwise_not(img1)\n",
    "bitnot2=cv2.bitwise_not(img2)\n",
    "\n",
    "cv2.imshow('image1',img1)\n",
    "cv2.imshow('image2',img2)\n",
    "cv2.imshow('bitnot1',bitnot1)\n",
    "cv2.imshow('bitnot2',bitnot2)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    print(x)\n",
    "\n",
    "img=np.zeros((300,512,3),np.uint8)\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('B','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('G','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('R','image',0,255,nothing)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    print(x)\n",
    "\n",
    "img=np.zeros((300,512,3),np.uint8)\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('B','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('G','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('R','image',0,255,nothing)\n",
    "\n",
    "while(1):\n",
    "    cv2.imshow('image',img)\n",
    "    \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    b=cv2.getTrackbarPos('B','image')\n",
    "    g=cv2.getTrackbarPos('G','image')\n",
    "    r=cv2.getTrackbarPos('R','image')\n",
    "    \n",
    "    \n",
    "    img[:]=[b,g,r]\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    print(x)\n",
    "\n",
    "img=np.zeros((300,512,3),np.uint8)\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('B','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('G','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('R','image',0,255,nothing)\n",
    "\n",
    "switch='0:OFF\\n 1:ON'\n",
    "\n",
    "cv2.createTrackbar(switch,'image',0,1,nothing)\n",
    "\n",
    "while(1):\n",
    "    cv2.imshow('image',img)\n",
    "    \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    b=cv2.getTrackbarPos('B','image')\n",
    "    g=cv2.getTrackbarPos('G','image')\n",
    "    r=cv2.getTrackbarPos('R','image')\n",
    "    s=cv2.getTrackbarPos(switch,'image')\n",
    "    \n",
    "    if s==0:\n",
    "        img[:]=0\n",
    "    else:\n",
    "        img[:]=[b,g,r]\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    print(x)\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('CP','image',10,400,nothing)\n",
    "\n",
    "\n",
    "\n",
    "switch='color/gray'\n",
    "\n",
    "cv2.createTrackbar(switch,'image',0,1,nothing)\n",
    "\n",
    "while(1):\n",
    "    img=cv2.imread('lena.jpg')\n",
    "    pos=cv2.getTrackbarPos('CP','image')\n",
    "    font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img,str(pos),(50,150),font,4,(0,0,255),10,6)\n",
    "    \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "        \n",
    "        \n",
    "   \n",
    "    s=cv2.getTrackbarPos(switch,'image')\n",
    "   \n",
    "    \n",
    "    if s==0:\n",
    "        pass\n",
    "    else:\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    img=cv2.imshow('image',img)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#cv2.namedWindow('Tracking')\n",
    "\n",
    "img=cv2.imread('smarties.png')\n",
    "hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "l_b=np.array([110,50,50])\n",
    "u_b=np.array([130,255,255])\n",
    "\n",
    "mask=cv2.inRange(hsv,l_b,u_b)\n",
    "\n",
    "res=cv2.bitwise_and(img,img,mask=mask)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.imshow('mask',mask)\n",
    "cv2.imshow('res',res)\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "cv2.namedWindow('Tracking')\n",
    "cv2.createTrackbar('LH','Tracking',0,255,nothing())\n",
    "cv2.createTrackbar('LS','Tracking',0,255,nothing())\n",
    "cv2.createTrackbar('LV','Tracking',0,255,nothing())\n",
    "cv2.createTrackbar('UH','Tracking',255,255,nothing())\n",
    "cv2.createTrackbar('US','Tracking',255,255,nothing())\n",
    "cv2.createTrackbar('UV','Tracking',255,255,nothing())\n",
    "\n",
    "\n",
    "img=cv2.imread('smarties.png')\n",
    "hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "l_h=cv2.getTrackbarPos('LH','Tracking')\n",
    "l_s=cv2.getTrackbarPos('LS','Tracking')\n",
    "l_v=cv2.getTrackbarPos('LV','Tracking')\n",
    "\n",
    "u_h=cv2.getTrackbarPos('UH','Tracking')\n",
    "u_s=cv2.getTrackbarPos('US','Tracking')\n",
    "u_v=cv2.getTrackbarPos('UV','Tracking')\n",
    "\n",
    "l_b=np.array([l_h,l_s,l_v])\n",
    "u_b=np.array([u_h,u_s,u_v])\n",
    "\n",
    "mask=cv2.inRange(hsv,l_b,u_b)\n",
    "\n",
    "res=cv2.bitwise_and(img,img,mask=mask)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.imshow('mask',mask)\n",
    "cv2.imshow('res',res)\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow('Tracking')\n",
    "cv2.createTrackbar('LH','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('LS','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('LV','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('UH','Tracking',255,255,nothing)\n",
    "cv2.createTrackbar('US','Tracking',255,255,nothing)\n",
    "cv2.createTrackbar('UV','Tracking',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    #img=cv2.imread('smarties.png')\n",
    "    \n",
    "    _,video=cap.read()\n",
    "    \n",
    "    hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    l_h=cv2.getTrackbarPos('LH','Tracking')\n",
    "    l_s=cv2.getTrackbarPos('LS','Tracking')\n",
    "    l_v=cv2.getTrackbarPos('LV','Tracking')\n",
    "\n",
    "    u_h=cv2.getTrackbarPos('UH','Tracking')\n",
    "    u_s=cv2.getTrackbarPos('US','Tracking')\n",
    "    u_v=cv2.getTrackbarPos('UV','Tracking')\n",
    "\n",
    "    l_b=np.array([l_h,l_s,l_v])\n",
    "    u_b=np.array([u_h,u_s,u_v])\n",
    "\n",
    "    mask=cv2.inRange(hsv,l_b,u_b)\n",
    "\n",
    "    res=cv2.bitwise_and(img,img,mask=mask)\n",
    "\n",
    "    cv2.imshow('image',img)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "cap.release()        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hue,Saturation and Value operations on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "#cap=cv2.VideoCapture(0);\n",
    "\n",
    "cv2.namedWindow('Tracking')\n",
    "cv2.createTrackbar('LH','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('LS','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('LV','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('UH','Tracking',255,255,nothing)\n",
    "cv2.createTrackbar('US','Tracking',255,255,nothing)\n",
    "cv2.createTrackbar('UV','Tracking',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    img=cv2.imread('smarties.png')\n",
    "    \n",
    "    _, frame=cap.read()\n",
    "    \n",
    "    hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    l_h=cv2.getTrackbarPos('LH','Tracking')\n",
    "    l_s=cv2.getTrackbarPos('LS','Tracking')\n",
    "    l_v=cv2.getTrackbarPos('LV','Tracking')\n",
    "\n",
    "    u_h=cv2.getTrackbarPos('UH','Tracking')\n",
    "    u_s=cv2.getTrackbarPos('US','Tracking')\n",
    "    u_v=cv2.getTrackbarPos('UV','Tracking')\n",
    "\n",
    "    l_b=np.array([l_h,l_s,l_v])\n",
    "    u_b=np.array([u_h,u_s,u_v])\n",
    "\n",
    "    mask=cv2.inRange(hsv,l_b,u_b)\n",
    "\n",
    "    res=cv2.bitwise_and(img,img,mask=mask)\n",
    "\n",
    "    cv2.imshow('image',img)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "cap.release()        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hue,Saturation and Value operations on live video capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0);\n",
    "\n",
    "cv2.namedWindow('Tracking')\n",
    "cv2.createTrackbar('LH','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('LS','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('LV','Tracking',0,255,nothing)\n",
    "cv2.createTrackbar('UH','Tracking',255,255,nothing)\n",
    "cv2.createTrackbar('US','Tracking',255,255,nothing)\n",
    "cv2.createTrackbar('UV','Tracking',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    #img=cv2.imread('smarties.png')\n",
    "    \n",
    "    _, frame=cap.read()\n",
    "    \n",
    "    hsv=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    l_h=cv2.getTrackbarPos('LH','Tracking')\n",
    "    l_s=cv2.getTrackbarPos('LS','Tracking')\n",
    "    l_v=cv2.getTrackbarPos('LV','Tracking')\n",
    "\n",
    "    u_h=cv2.getTrackbarPos('UH','Tracking')\n",
    "    u_s=cv2.getTrackbarPos('US','Tracking')\n",
    "    u_v=cv2.getTrackbarPos('UV','Tracking')\n",
    "\n",
    "    l_b=np.array([l_h,l_s,l_v])\n",
    "    u_b=np.array([u_h,u_s,u_v])\n",
    "\n",
    "    mask=cv2.inRange(hsv,l_b,u_b)\n",
    "\n",
    "    res=cv2.bitwise_and(frame,frame,mask=mask)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "cap.release()    \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img=cv2.imread('gradient.png',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,110,255,cv2.THRESH_BINARY)\n",
    "\n",
    "_,th2=cv2.threshold(img,110,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "_,th3=cv2.threshold(img,100,255,cv2.THRESH_TRUNC)\n",
    "\n",
    "_,th4=cv2.threshold(img,110,255,cv2.THRESH_TOZERO)\n",
    "\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.imshow('th1',th1)\n",
    "cv2.imshow('th2',th2)\n",
    "cv2.imshow('th3',th3)\n",
    "cv2.imshow('th4',th4)\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of edge of an image using thresholding technqiues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image=cv2.imread('aadhar card.jpg')\n",
    "\n",
    "cv2.imshow('original image',image)\n",
    "\n",
    "img=cv2.imread('aadhar card.jpg',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,110,255,cv2.THRESH_BINARY)\n",
    "\n",
    "th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.imshow('th1',th1)\n",
    "cv2.imshow('th2',th2)\n",
    "cv2.imshow('th3',th3)\n",
    "\n",
    "\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "img=cv2.imread('lena.jpg',1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img)\n",
    "#plt.xticks([])\n",
    "#plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image thresholding using various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "img=cv2.imread('65019.jpg',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,110,255,cv2.THRESH_BINARY)\n",
    "\n",
    "_,th2=cv2.threshold(img,110,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "_,th3=cv2.threshold(img,100,255,cv2.THRESH_TRUNC)\n",
    "\n",
    "_,th4=cv2.threshold(img,110,255,cv2.THRESH_TOZERO)\n",
    "\n",
    "_,th5=cv2.threshold(img,110,255,cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "titles=['Original Image','Binary','Binary Inv','Trunc','Tozero','Tozero Inv']\n",
    "\n",
    "images=[img,th1,th2,th3,th4,th5]\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological transformation on binary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "img=cv2.imread('smarties.png',0)\n",
    "\n",
    "_,mask=cv2.threshold(img,220,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "\n",
    "kernal=np.ones((5,5),np.uint8)\n",
    "\n",
    "dilation=cv2.dilate(mask,kernal,iterations=2)\n",
    "\n",
    "erosion=cv2.erode(mask,kernal,iterations=1)\n",
    "\n",
    "opening=cv2.morphologyEx(mask,cv2.MORPH_OPEN,kernal)\n",
    "\n",
    "closing=cv2.morphologyEx(mask,cv2.MORPH_CLOSE,kernal)\n",
    "\n",
    "mg=cv2.morphologyEx(mask,cv2.MORPH_GRADIENT,kernal)\n",
    "\n",
    "th=cv2.morphologyEx(mask,cv2.MORPH_TOPHAT,kernal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "titles=['Original Image','mask','dilation','erosion','opening','closing','mg','th']\n",
    "\n",
    "images=[img,mask,dilation,erosion,opening,closing,mg,th]\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    plt.subplot(2,4,i+1)\n",
    "    \n",
    "    plt.imshow(images[i],'gray')\n",
    "    \n",
    "    plt.title(titles[i])\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dilation,erode,morphology ooperations on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "img=cv2.imread('lena.jpg',0)\n",
    "\n",
    "#_,mask=cv2.threshold(img,220,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "\n",
    "kernal=np.ones((5,5),np.uint8)\n",
    "\n",
    "dilation=cv2.dilate(img,kernal,iterations=2)\n",
    "\n",
    "erosion=cv2.erode(img,kernal,iterations=1)\n",
    "\n",
    "opening=cv2.morphologyEx(img,cv2.MORPH_OPEN,kernal)\n",
    "\n",
    "closing=cv2.morphologyEx(img,cv2.MORPH_CLOSE,kernal)\n",
    "\n",
    "mg=cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernal)\n",
    "\n",
    "th=cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "titles=['Original Image','mask','dilation','erosion','opening','closing','mg','th']\n",
    "\n",
    "images=[img,img,dilation,erosion,opening,closing,mg,th]\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    plt.subplot(2,4,i+1)\n",
    "    \n",
    "    plt.imshow(images[i],'gray')\n",
    "    \n",
    "    plt.title(titles[i])\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image smoothing/blurring using different filters(linear filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=cv2.imread('lena.jpg')\n",
    "\n",
    "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "kernel=np.ones((5,5),np.float32)/25\n",
    "\n",
    "dst=cv2.filter2D(img,-1,kernel)\n",
    "\n",
    "blurr=cv2.blur(img,(5,5))\n",
    "\n",
    "gblur=cv2.GaussianBlur(img,(5,5),0)\n",
    "\n",
    "median=cv2.medianBlur(img,5)\n",
    "\n",
    "bfilter=cv2.bilateralFilter(img,9,75,75)\n",
    "\n",
    "\n",
    "titles=['Original Image','2D convolution','blurr','gblur','median','bfilter']\n",
    "\n",
    "images=[img,dst,blurr,gblur,median,bfilter]\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    plt.subplot(2,3,i+1)\n",
    "    \n",
    "    plt.imshow(images[i],'gray')\n",
    "    \n",
    "    plt.title(titles[i])\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edge detection using sobel,laplacian detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=cv2.imread('messi5.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "lap=cv2.Laplacian(img,cv2.CV_64F,ksize=3)\n",
    "\n",
    "lap=np.uint8(np.absolute(lap))\n",
    "\n",
    "sobelx=cv2.Sobel(img,cv2.CV_64F, 1,0)\n",
    "\n",
    "sobely=cv2.Sobel(img,cv2.CV_64F, 0,1)\n",
    "\n",
    "sobelx=np.uint8(np.absolute(sobelx))\n",
    "\n",
    "sobely=np.uint8(np.absolute(sobely))\n",
    "\n",
    "sobel=cv2.bitwise_or(sobelx,sobely)\n",
    "\n",
    "edge=cv2.Canny(img,100,200)\n",
    "\n",
    "\n",
    "titles=['Original Image','Laplacian','sobelx','sobely','sobel','edge']\n",
    "\n",
    "images=[img,lap,sobelx,sobely,sobel,edge]\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    plt.subplot(2,3,i+1)\n",
    "    \n",
    "    plt.imshow(images[i],'gray')\n",
    "    \n",
    "    plt.title(titles[i])\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt\n",
    "img=cv2.imread('lena.jpg',0)\n",
    "\n",
    "canny=cv2.Canny(img,100,200)\n",
    "\n",
    "\n",
    "\n",
    "titles=['Original Image','canny']\n",
    "\n",
    "images=[img,canny]\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    plt.subplot(1,2,i+1)\n",
    "    \n",
    "    plt.imshow(images[i],'gray')\n",
    "    \n",
    "    plt.title(titles[i])\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# canny edge detection of lena image with trackbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('th1','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('th2','image',255,255,nothing)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    img=cv2.imread('78019.jpg')\n",
    "    \n",
    "               \n",
    "    th1=cv2.getTrackbarPos('th1','image')\n",
    "    th2=cv2.getTrackbarPos('th2','image')\n",
    "    edge=cv2.Canny(img,th1,th2)\n",
    "        \n",
    "    cv2.imshow('image',img)\n",
    "    \n",
    "    cv2.imshow('canny',edge)\n",
    "   \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# canny edge detection of aadhar card using trackbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('th1','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('th2','image',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    img=cv2.imread('65019.jpg')\n",
    "    \n",
    "    th1=cv2.getTrackbarPos('th1','image')\n",
    "    \n",
    "    th2=cv2.getTrackbarPos('th2','image')\n",
    "    \n",
    "    edge=cv2.Canny(img,th1,th2) \n",
    "\n",
    "    \n",
    "    cv2.imshow('image',img)\n",
    "    cv2.imshow('edge',edge)\n",
    "    \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lena image pyramid down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=cv2.imread('lena.jpg')\n",
    "\n",
    "layer=img.copy()\n",
    "\n",
    "gp=[layer]\n",
    "\n",
    "for i in range(6):\n",
    "    layer=cv2.pyrDown(layer)\n",
    "    \n",
    "    gp.append(layer)\n",
    "    \n",
    "    cv2.imshow(str(i),layer)\n",
    "    \n",
    "    \n",
    "\n",
    "cv2.imshow('original image',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "apple=cv2.imread('apple.jpg')\n",
    "\n",
    "orange=cv2.imread('orange.jpg')\n",
    "\n",
    "apple_orange=np.hstack((apple[:,:256],orange[:,256:]))\n",
    "\n",
    "print(apple.shape)\n",
    "print(orange.shape)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow('apple',apple)\n",
    "cv2.imshow('orange',orange)\n",
    "cv2.imshow('apple_orange',apple_orange)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=cv2.imread('opencv-logo.png')\n",
    "\n",
    "imgray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret,thresh=cv2.threshold(imgray,127,255,0)\n",
    "\n",
    "contours,heirarchy=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "print('Number of Contours=' + str(len(contours)))\n",
    "\n",
    "print(contours[0])\n",
    "\n",
    "cv2.drawContours(img,contours,-1,(0,255,255),3)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.imshow('imgage GRAY',imgray)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap=cv2.VideoCapture('vtest.avi')\n",
    "\n",
    "ret,frame1=cap.read()\n",
    "\n",
    "ret,frame2=cap.read()\n",
    "\n",
    "while cap.isOpened():\n",
    "    \n",
    "    diff=cv2.absdiff(frame1,frame2)\n",
    "    \n",
    "    gray=cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    blur=cv2.GaussianBlur(gray,(5,5),0)\n",
    "    \n",
    "    _,thresh=cv2.threshold(blur,20,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "    dilated=cv2.dilate(thresh,None,3)\n",
    "    \n",
    "    contours,_=cv2.findContours(dilated,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        (x,y,w,h)=cv2.boundingRect(contour)\n",
    "        \n",
    "        if cv2.contourArea(contour)<700:\n",
    "            continue\n",
    "        cv2.rectangle(frame1,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        \n",
    "        cv2.putText(frame1,'status:{}'.format('movement'),(10,20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "    #cv2.drawContours(frame1,contours,-1,(0,255,0),2)\n",
    "    \n",
    "    cv2.imshow('feed',frame1)\n",
    "    \n",
    "    frame1=frame2\n",
    "    \n",
    "    ret,frame2=cap.read()\n",
    "    \n",
    "    if cv2.waitKey(40)==27:\n",
    "        \n",
    "        break\n",
    "        \n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=cv2.imread('shapes.jpg')\n",
    "\n",
    "imgrey=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_,thresh=cv2.threshold(imgrey,240,255,cv2.THRESH_BINARY)\n",
    "\n",
    "contours,_=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "for contour in contours:\n",
    "    approx=cv2.approxPolyDP(contour,0.01*cv2.arcLength(contour,True),True)\n",
    "    \n",
    "    cv2.drawContours(img,[approx],0,(0,0,0),5)\n",
    "    \n",
    "    x=approx.ravel()[0]\n",
    "    \n",
    "    y=approx.ravel()[1]-8\n",
    "    \n",
    "    if len(approx)==3:\n",
    "        cv2.putText(img,'triangle',(x,y),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0))\n",
    "    elif len(approx)==4:\n",
    "        x,y,w,h=cv2.boundingRect(approx)\n",
    "        \n",
    "        aspectRatio=float(w)/h\n",
    "        \n",
    "        print(aspectRatio)\n",
    "        \n",
    "        if aspectRatio>=0.95 and  aspectRatio<=1.05:\n",
    "            \n",
    "            cv2.putText(img,'square',(x,y),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0))\n",
    "        else:\n",
    "            cv2.putText(img,'rectangle',(x,y),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0))\n",
    "            \n",
    "    elif len(approx)==5:\n",
    "        cv2.putText(img,'pentagon',(x,y),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0))\n",
    "    elif len(approx)==10:\n",
    "        cv2.putText(img,'star',(x,y),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0))\n",
    "    else:\n",
    "         cv2.putText(img,'circle',(x,y),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0))\n",
    "    \n",
    "        \n",
    "\n",
    "cv2.imshow('image',img)\n",
    "#cv2.imshow('imgrey',imgrey)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open cv Module Histogram on 18 nov.2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "%matplotlib qt\n",
    "\n",
    "img=np.zeros((200,200),np.uint8)\n",
    "\n",
    "cv2.rectangle(img,(0,100),(200,200),(255),-1)\n",
    "\n",
    "cv2.rectangle(img,(0,50),(100,100),(127),-1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "plt.hist(img.ravel(),256,[0,256])\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "%matplotlib qt\n",
    "\n",
    "img=cv2.imread('lena.jpg')\n",
    "\n",
    "hist=cv2.calcHist([img],[0],None,[256],[0,256])\n",
    "\n",
    "plt.plot(hist)\n",
    "\n",
    "#b, g, r=cv2.split(img)\n",
    "\n",
    "#cv2.imshow('image',img)\n",
    "#cv2.imshow('b',b)\n",
    "#cv2.imshow('g',g)\n",
    "#cv2.imshow('r',r)\n",
    "\n",
    "#plt.hist(b.ravel(),256,[0,256])\n",
    "#plt.hist(g.ravel(),256,[0,256])\n",
    "#plt.hist(r.ravel(),256,[0,256])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "%matplotlib qt\n",
    "\n",
    "img=cv2.imread('messi5.jpg')\n",
    "\n",
    "grey_img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "template=cv2.imread('messi_face.png',0)\n",
    "\n",
    "w,h=template.shape[::-1]\n",
    "\n",
    "res=cv2.matchTemplate(grey_img,template,cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "print(res)\n",
    "\n",
    "threshold=0.95;\n",
    "\n",
    "loc=np.where(res>=threshold)\n",
    "\n",
    "print(loc)\n",
    "\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(img,pt,(pt[0]+w,pt[1]+h),(0,255,0),2)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=cv2.imread('sudoku.png')\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges=cv2.Canny(gray,50,150,apertureSize=3)\n",
    "\n",
    "cv2.imshow('edges',edges)\n",
    "\n",
    "lines=cv2.HoughLines(edges,1,np.pi/180,200)\n",
    "\n",
    "for line in lines:\n",
    "    rho,theta=line[0]\n",
    "    a=np.cos(theta)\n",
    "    b=np.sin(theta)\n",
    "    \n",
    "    x0=a*rho\n",
    "    y0=b*rho\n",
    "    \n",
    "    x1=int(x0+1000*(-b))\n",
    "    \n",
    "    y1=int(y0+1000*(a))\n",
    "    \n",
    "    x2=int(x0-1000*(-b))\n",
    "    \n",
    "    y2=int(y0-1000*(a))\n",
    "    \n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "    \n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=cv2.imread('road.jpg')\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges=cv2.Canny(gray,150,250,apertureSize=3)\n",
    "\n",
    "cv2.imshow('edges',edges)\n",
    "\n",
    "lines=cv2.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\n",
    "\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2= line[0]\n",
    "    \n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "    \n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lane line detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def region_of_interest(img,vertices):\n",
    "    \n",
    "    mask=np.zeros_like(img)\n",
    "    #channel_count=img.shape[2]\n",
    "    match_mask_color=255\n",
    "    cv2.fillPoly(mask,vertices,match_mask_color)\n",
    "    masked_image=cv2.bitwise_and(img,mask)\n",
    "    return masked_image\n",
    "\n",
    "def draw_the_lines(img,lines):\n",
    "    img=np.copy(img)\n",
    "    blank_image=np.zeros((img.shape[0],img.shape[1],3),dtype=np.uint8)\n",
    "    \n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(blank_image,(x1,y1),(x2,y2),(0,255,0),3)\n",
    "    img=cv2.addWeighted(img,0.8,blank_image,1,0.0)\n",
    "    return img\n",
    "        \n",
    "\n",
    "image=cv2.imread('road.jpg')\n",
    "image=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "height=image.shape[0]\n",
    "\n",
    "width=image.shape[1]\n",
    "\n",
    "region_of_interest_vertices=[\n",
    "    (0,height),(width/2,height/2),(width,height)\n",
    "]\n",
    "gray_image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "canny_image=cv2.Canny(gray_image,100,200)\n",
    "\n",
    "cropped_image=region_of_interest(canny_image,np.array([region_of_interest_vertices],np.int32))\n",
    "\n",
    "\n",
    "lines=cv2.HoughLinesP(cropped_image,6,np.pi/60,160,None,minLineLength=40,maxLineGap=25)\n",
    "\n",
    "image_with_line=draw_the_lines(image,lines)  \n",
    "plt.imshow(image_with_line)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def region_of_interest(img,vertices):\n",
    "    \n",
    "    mask=np.zeros_like(img)\n",
    "    #channel_count=img.shape[2]\n",
    "    match_mask_color=255\n",
    "    cv2.fillPoly(mask,vertices,match_mask_color)\n",
    "    masked_image=cv2.bitwise_and(img,mask)\n",
    "    return masked_image\n",
    "\n",
    "def draw_the_lines(img,lines):\n",
    "    img=np.copy(img)\n",
    "    blank_image=np.zeros((img.shape[0],img.shape[1],3),dtype=np.uint8)\n",
    "    \n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(blank_image,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "    img=cv2.addWeighted(img,0.8,blank_image,1,0.0)\n",
    "    return img\n",
    "        \n",
    "\n",
    "image=cv2.imread('road.jpg')\n",
    "image=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "height=image.shape[0]\n",
    "\n",
    "width=image.shape[1]\n",
    "\n",
    "region_of_interest_vertices=[\n",
    "    (0,height),(width/2,height/2),(width,height)\n",
    "]\n",
    "gray_image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "canny_image=cv2.Canny(gray_image,100,200)\n",
    "\n",
    "cropped_image=region_of_interest(canny_image,np.array([region_of_interest_vertices],np.int32))\n",
    "\n",
    "\n",
    "lines=cv2.HoughLinesP(cropped_image,6,np.pi/60,160,None,minLineLength=40,maxLineGap=25)\n",
    "\n",
    "image_with_lines=draw_the_lines(image,lines) \n",
    "\n",
    "plt.imshow(image_with_lines)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def drow_the_lines(img, lines):\n",
    "    img = np.copy(img)\n",
    "    blank_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            cv2.line(blank_image, (x1,y1), (x2,y2), (255, 0, 0), thickness=10)\n",
    "\n",
    "    img = cv2.addWeighted(img, 0.8, blank_image, 1, 0.0)\n",
    "    return img\n",
    "\n",
    "image = cv2.imread('road.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "region_of_interest_vertices = [\n",
    "    (0, height),\n",
    "    (width/2, height/2),\n",
    "    (width, height)\n",
    "]\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "canny_image = cv2.Canny(gray_image, 100, 200)\n",
    "cropped_image = region_of_interest(canny_image,\n",
    "                np.array([region_of_interest_vertices], np.int32),)\n",
    "lines = cv2.HoughLinesP(cropped_image,\n",
    "                        rho=6,\n",
    "                        theta=np.pi/180,\n",
    "                        threshold=160,\n",
    "                        lines=np.array([]),\n",
    "                        minLineLength=40,\n",
    "                        maxLineGap=25)\n",
    "image_with_lines = drow_the_lines(image, lines)\n",
    "plt.imshow(image_with_lines)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def drow_the_lines(img, lines):\n",
    "    img = np.copy(img)\n",
    "    blank_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            cv2.line(blank_image, (x1,y1), (x2,y2), (255, 0, 0), thickness=10)\n",
    "\n",
    "    img = cv2.addWeighted(img, 0.8, blank_image, 1, 0.0)\n",
    "    return img\n",
    "\n",
    "def process(image):\n",
    "    print(image.shape)\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    region_of_interest_vertices = [\n",
    "        (0, height),\n",
    "        (width/2, height/2),\n",
    "        (width, height)\n",
    "    ]\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    canny_image = cv2.Canny(gray_image, 100, 200)\n",
    "    cropped_image = region_of_interest(canny_image,\n",
    "                    np.array([region_of_interest_vertices], np.int32),)\n",
    "    lines = cv2.HoughLinesP(cropped_image,\n",
    "                            rho=6,\n",
    "                            theta=np.pi/180,\n",
    "                            threshold=160,\n",
    "                            lines=np.array([]),\n",
    "                            minLineLength=40,\n",
    "                            maxLineGap=25)\n",
    "    image_with_lines = drow_the_lines(image, lines)\n",
    "    return image_with_lines\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture('test_video.mp4')\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret,frame=cap.read()\n",
    "    frame=process(frame)\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def drow_the_lines(img, lines):\n",
    "    img = np.copy(img)\n",
    "    blank_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            cv2.line(blank_image, (x1,y1), (x2,y2), (0, 255, 0), thickness=10)\n",
    "\n",
    "    img = cv2.addWeighted(img, 0.8, blank_image, 1, 0.0)\n",
    "    return img\n",
    "\n",
    "# = cv2.imread('road.jpg')\n",
    "#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "def process(image):\n",
    "    print(image.shape)\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    region_of_interest_vertices = [\n",
    "        (0, height),\n",
    "        (width/2, height/2),\n",
    "        (width, height)\n",
    "    ]\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    canny_image = cv2.Canny(gray_image, 100, 120)\n",
    "    cropped_image = region_of_interest(canny_image,\n",
    "                    np.array([region_of_interest_vertices], np.int32),)\n",
    "    lines = cv2.HoughLinesP(cropped_image,\n",
    "                            rho=2,\n",
    "                            theta=np.pi/180,\n",
    "                            threshold=50,\n",
    "                            lines=np.array([]),\n",
    "                            minLineLength=40,\n",
    "                            maxLineGap=100)\n",
    "    image_with_lines = drow_the_lines(image, lines)\n",
    "    return image_with_lines\n",
    "\n",
    "cap = cv2.VideoCapture('test_video.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = process(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "img=cv2.imread('shapes.jpg')\n",
    "output=img.copy()\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray=cv2.medianBlur(gray,5)\n",
    "\n",
    "circles=cv2.HoughCircles(gray,cv2.HOUGH_GRADIENT,1,20,param1=50,param2=30,minRadius=0,maxRadius=0)\n",
    "\n",
    "detected_circles=np.uint16(np.around(circles))\n",
    "for (x,y,r) in detected_circles[0,:]:\n",
    "    cv2.circle(output,(x,y),r,(255,255,0),3)\n",
    "    cv2.circle(output,(x,y),0,(0,255,255),3)\n",
    "\n",
    "\n",
    "cv2.imshow('output',output)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face detection in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "face_cascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "img=cv2.imread('naveen.jpg')\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "faces=face_cascade.detectMultiScale(gray,1.1,4)\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),3)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face detection in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "face_cascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap=cv2.VideoCapture('test.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _, img= cap.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray,1.1,4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),3)\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "\n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()   \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('test.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _, img = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y , w ,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)\n",
    "        roi_gray=gray[y:y+h,x:x+w]\n",
    "        roi_color=img[y:y+h,x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),5)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('chessboard.png')\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray, 30, 3, 0.04)\n",
    "\n",
    "dst = cv2.dilate(dst, None)\n",
    "\n",
    "img[dst > 0.01 * dst.max()] = [0, 0, 255]\n",
    "\n",
    "cv2.imshow('dst', img)\n",
    "\n",
    "if cv2.waitKey(0) & 0xff == 27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('pic1.png')\n",
    "\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "corners = cv.goodFeaturesToTrack(gray, 100, 0.01, 10)\n",
    "\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv.circle(img, (x, y), 3, 255, -1)\n",
    "\n",
    "cv.imshow('image', img)\n",
    "\n",
    "if cv.waitKey(0) & 0xff == 27:\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('th1','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('th2','image',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    img=cv2.imread('aadhar card.jpg')\n",
    "    \n",
    "    th1=cv2.getTrackbarPos('th1','image')\n",
    "    \n",
    "    th2=cv2.getTrackbarPos('th2','image')\n",
    "\n",
    "    edge=cv2.Canny(img, th1, th2)\n",
    "    \n",
    "    cv2.imshow('image',img)\n",
    "    cv2.imshow('edge',edge)\n",
    "\n",
    "    text=image_to_string(edge)\n",
    "    print (text)\n",
    "    \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import image_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "image=cv2.imread('aadhar card.jpg')\n",
    "\n",
    "cv2.imshow('original image',image)\n",
    "\n",
    "img=cv2.imread('aadhar card.jpg',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,110,255,cv2.THRESH_BINARY)\n",
    "\n",
    "#th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "#th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.imshow('th1',th1)\n",
    "#cv2.imshow('th2',th2)\n",
    "#cv2.imshow('th3',th3)\n",
    "\n",
    "text=image_to_string(th1)\n",
    "print (text)\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edge detection using canny,sobel,prewitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('messi5.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img_gaussian = cv2.GaussianBlur(gray,(3,3),0)\n",
    "\n",
    "#canny\n",
    "img_canny = cv2.Canny(img,100,200)\n",
    "\n",
    "#sobel\n",
    "img_sobelx = cv2.Sobel(img_gaussian,cv2.CV_8U,1,0,ksize=5)\n",
    "img_sobely = cv2.Sobel(img_gaussian,cv2.CV_8U,0,1,ksize=5)\n",
    "img_sobel = img_sobelx + img_sobely\n",
    "\n",
    "\n",
    "#prewitt\n",
    "kernelx = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])\n",
    "kernely = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])\n",
    "img_prewittx = cv2.filter2D(img_gaussian, -1, kernelx)\n",
    "img_prewitty = cv2.filter2D(img_gaussian, -1, kernely)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Canny\", img_canny)\n",
    "cv2.imshow(\"Sobel X\", img_sobelx)\n",
    "cv2.imshow(\"Sobel Y\", img_sobely)\n",
    "cv2.imshow(\"Sobel\", img_sobel)\n",
    "cv2.imshow(\"Prewitt X\", img_prewittx)\n",
    "cv2.imshow(\"Prewitt Y\", img_prewitty)\n",
    "cv2.imshow(\"Prewitt\", img_prewittx + img_prewitty)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "for f in os.listdir('.'):\n",
    "    if f.endswith('.jpg'):\n",
    "        print (f)\n",
    "\n",
    "\n",
    "img=Image.open('lena.jpg')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "image=cv2.imread('tfs2.png')\n",
    "\n",
    "cv2.imshow('original image',image)\n",
    "\n",
    "img=cv2.imread('tfs2.png',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,135,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "edge=cv2.Canny(img,135,255)\n",
    "\n",
    "# cv2.imshow('image',img)\n",
    "\n",
    "# cv2.imshow('th1',th1)\n",
    "# cv2.imshow('th2',th2)\n",
    "# cv2.imshow('th3',th3)\n",
    "cv2.imshow('edge',edge)\n",
    "\n",
    "text=image_to_string(image,lang='eng+hin')\n",
    "print (text)\n",
    "#f=open(\"pan2.txt\",\"w+\")\n",
    "#f.write(text)\n",
    "#f.close()\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import image_to_string\n",
    "from PIL import Image\n",
    "img1 = Image.open('aadhar card.jpg')\n",
    "txt = pytesseract.image_to_string(img1, lang='hin+eng')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# teseract-ocr for text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL.Image\n",
    "\n",
    "from pytesseract import image_to_string\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:/Users/naveen.dagar/AppData/Local/Tesseract-OCR/tesseract.exe\"\n",
    "TESSDATA_PREFIX = \"C:/Users/naveen.dagar/AppData/Local/Tesseract-OCR\"\n",
    "output = pytesseract.image_to_string(PIL.Image.open(\"C:/Users/naveen.dagar/start learning python/hin.jpg\"), lang='eng+hin')\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different techniques of edge detection on 20 nov 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('th1','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('th2','image',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    img=cv2.imread('sbi.png')\n",
    "    \n",
    "    th1=cv2.getTrackbarPos('th1','image')\n",
    "    \n",
    "    th2=cv2.getTrackbarPos('th2','image')\n",
    "    \n",
    "    edge=cv2.Canny(img,th1,th2) \n",
    "\n",
    "    \n",
    "    cv2.imshow('image',img)\n",
    "    cv2.imshow('edge',edge)\n",
    "    \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#from pytesseract import image_to_string\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('lt','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('ut','image',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    img=cv2.imread('aadhar card.jpg')\n",
    "    \n",
    "    lt=cv2.getTrackbarPos('lt','image')\n",
    "    \n",
    "    ut=cv2.getTrackbarPos('ut','image')\n",
    "\n",
    "#image=cv2.imread('aadhar card.jpg')\n",
    "\n",
    "#cv2.imshow('original image',image)\n",
    "\n",
    "#img=cv2.imread('aadhar card.jpg',0)\n",
    "\n",
    "    _,th1=cv2.threshold(img,lt,ut,cv2.THRESH_BINARY)\n",
    "    #edge=cv2.Canny(img,lt,ut) \n",
    "\n",
    "\n",
    "#th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "#th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "\n",
    "    cv2.imshow('image',img)\n",
    "\n",
    "    cv2.imshow('th1',th1)\n",
    "#cv2.imshow('th2',th2)\n",
    "#cv2.imshow('th3',th3)\n",
    "\n",
    "#text=image_to_string(img)\n",
    "#print (text)\n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "\n",
    "cv2.createTrackbar('th1','image',0,255,nothing)\n",
    "\n",
    "cv2.createTrackbar('th2','image',255,255,nothing)\n",
    "\n",
    "while True:\n",
    "    img=cv2.imread('pan1.jpg',0)\n",
    "    \n",
    "    th1=cv2.getTrackbarPos('th1','image')\n",
    "    \n",
    "    th2=cv2.getTrackbarPos('th2','image')\n",
    "    \n",
    "    _,thr=cv2.threshold(img,th1,th2,cv2.THRESH_BINARY)\n",
    "    \n",
    "    #edge=cv2.Canny(img,th1,th2) \n",
    "\n",
    "    \n",
    "    cv2.imshow('image',img)\n",
    "    #cv2.imshow('edge',edge)\n",
    "    cv2.imshow('thr',thr)\n",
    "    \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.open('pan2.jfif')\n",
    "im.save('pan0.jpg')  # or 'test.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# storing the pancard extracted details into text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pytesseract import image_to_string\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "image=cv2.imread('tfs1.png')\n",
    "\n",
    "#cv2.imshow('original image',image)\n",
    "\n",
    "img=cv2.imread('tfs1.png',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,135,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "edge=cv2.Canny(img,135,255)\n",
    "\n",
    "#cv2.imshow('image',img)\n",
    "\n",
    "#cv2.imshow('th1',th1)\n",
    "#cv2.imshow('th2',th2)\n",
    "#cv2.imshow('th3',th3)\n",
    "#cv2.imshow('edge',edge)\n",
    "\n",
    "text=image_to_string(image,lang='eng+hin')\n",
    "print (text)\n",
    "file=open(\"tfs.txt\",\"w+\")\n",
    "#for i in text:\n",
    "with open(\"tfs.txt\",\"w+\", encoding = 'utf-8') as f:\n",
    "    f.write(text)\n",
    "file.close()\n",
    "k=cv2.waitKey(1)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "\n",
    "image=cv2.imread('chq1.png')\n",
    "\n",
    "cv2.imshow('original image',image)\n",
    "\n",
    "img=cv2.imread('chq1.png',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,135,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "edge=cv2.Canny(img,135,255)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.imshow('th1',th1)\n",
    "cv2.imshow('th2',th2)\n",
    "cv2.imshow('th3',th3)\n",
    "cv2.imshow('edge',edge)\n",
    "\n",
    "text=image_to_string(image,lang='eng+hin')\n",
    "print (text)\n",
    "file=open(\"chq1.txt\",\"w+\")\n",
    "with open(\"chq1.txt\",\"w+\", encoding = 'utf-8') as f:\n",
    "    f.write(text)\n",
    "file.close()\n",
    "k=cv2.waitKey(1)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#from pytesseract import image_to_string\n",
    "\n",
    "image=cv2.imread('chq6.jpg')\n",
    "\n",
    "cv2.imshow('original image',image)\n",
    "\n",
    "img=cv2.imread('chq6.jpg',0)\n",
    "\n",
    "_,th1=cv2.threshold(img,135,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "\n",
    "edge=cv2.Canny(img,135,255)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "\n",
    "cv2.imshow('th1',th1)\n",
    "cv2.imshow('th2',th2)\n",
    "cv2.imshow('th3',th3)\n",
    "cv2.imshow('edge',edge)\n",
    "\n",
    "#text=image_to_string(image,lang='eng+hin')\n",
    "print (text)\n",
    "file=open(\"chq6.txt\",\"w+\")\n",
    "with open(\"chq6.txt\",\"w+\", encoding = 'utf-8') as f:\n",
    "    f.write(text)\n",
    "#f=open(\"pan2.txt\",\"w+\")\n",
    "#f.write(text)\n",
    "#f.close()\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect the cheque number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check OCR: T011300142T 12345678U 0101\n"
     ]
    }
   ],
   "source": [
    "from skimage.segmentation import clear_border\n",
    "from imutils import contours\n",
    "import imutils\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "def extract_digits_and_symbols(image, charCnts, minW=5, minH=15):\n",
    "    \n",
    "    charIter = charCnts.__iter__()\n",
    "    rois = []\n",
    "    locs = []\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            c = next(charIter)\n",
    "            (cX, cY, cW, cH) = cv2.boundingRect(c)\n",
    "            roi = None\n",
    "\n",
    "            \n",
    "            if cW >= minW and cH >= minH:\n",
    "                # extract the ROI\n",
    "                roi = image[cY:cY + cH, cX:cX + cW]\n",
    "                rois.append(roi)\n",
    "                locs.append((cX, cY, cX + cW, cY + cH))\n",
    "\n",
    "            # otherwise, we are examining one of the special symbols\n",
    "            else:\n",
    "                \n",
    "                parts = [c, next(charIter), next(charIter)]\n",
    "                (sXA, sYA, sXB, sYB) = (np.inf, np.inf, -np.inf,\n",
    "                    -np.inf)\n",
    "\n",
    "                # loop over the parts\n",
    "                for p in parts:\n",
    "                    \n",
    "                    (pX, pY, pW, pH) = cv2.boundingRect(p)\n",
    "                    sXA = min(sXA, pX)\n",
    "                    sYA = min(sYA, pY)\n",
    "                    sXB = max(sXB, pX + pW)\n",
    "                    sYB = max(sYB, pY + pH)\n",
    "\n",
    "                # extract the ROI\n",
    "                roi = image[sYA:sYB, sXA:sXB]\n",
    "                rois.append(roi)\n",
    "                locs.append((sXA, sYA, sXB, sYB))\n",
    "\n",
    "        \n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    # return a tuple of the ROIs and locations\n",
    "    return (rois, locs)\n",
    "\n",
    "charNames = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\",\n",
    "    \"T\", \"U\", \"A\", \"D\"]\n",
    "\n",
    "\n",
    "ref = cv2.imread(\"C:/Users/naveen.dagar/start_learning_python/bank-check-ocr-part-ii/bank-check-ocr-2/micr_e13b_reference.png\")\n",
    "ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
    "ref = imutils.resize(ref, width=400)\n",
    "ref = cv2.threshold(ref, 0, 255, cv2.THRESH_BINARY_INV |\n",
    "    cv2.THRESH_OTSU)[1]\n",
    "\n",
    "refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "refCnts = imutils.grab_contours(refCnts)\n",
    "refCnts = contours.sort_contours(refCnts, method=\"left-to-right\")[0]\n",
    "\n",
    "refROIs = extract_digits_and_symbols(ref, refCnts,\n",
    "    minW=10, minH=20)[0]\n",
    "chars = {}\n",
    "\n",
    "# loop over the reference ROIs\n",
    "for (name, roi) in zip(charNames, refROIs):\n",
    "    \n",
    "    roi = cv2.resize(roi, (36, 36)) \n",
    "    chars[name] = roi\n",
    "\n",
    "rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 7))\n",
    "output = []\n",
    "\n",
    "\n",
    "image = cv2.imread(\"C:/Users/naveen.dagar/start_learning_python/bank-check-ocr-part-ii/bank-check-ocr-2/ch5.png\")\n",
    "(h, w,) = image.shape[:2]\n",
    "delta = int(h - (h * 0.2))\n",
    "bottom = image[delta:h, 0:w]\n",
    "\n",
    "\n",
    "gray = cv2.cvtColor(bottom, cv2.COLOR_BGR2GRAY)\n",
    "blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)\n",
    "\n",
    "\n",
    "gradX = cv2.Sobel(blackhat, ddepth=cv2.CV_32F, dx=1, dy=0,ksize=-1)\n",
    "gradX = np.absolute(gradX)\n",
    "(minVal, maxVal) = (np.min(gradX), np.max(gradX))\n",
    "gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))\n",
    "gradX = gradX.astype(\"uint8\")\n",
    "\n",
    "\n",
    "gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)\n",
    "thresh = cv2.threshold(gradX, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "thresh = clear_border(thresh)\n",
    "\n",
    "\n",
    "groupCnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "groupCnts = groupCnts[0]\n",
    "groupLocs = []\n",
    "\n",
    "# loop over the group contours\n",
    "for (i, c) in enumerate(groupCnts):\n",
    "    # compute the bounding box of the contour\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "    \n",
    "    if w > 20 and h > 15:\n",
    "        groupLocs.append((x, y, w, h))\n",
    "\n",
    "\n",
    "groupLocs = sorted(groupLocs, key=lambda x:x[0])\n",
    "\n",
    "\n",
    "for (gX, gY, gW, gH) in groupLocs:\n",
    "    \n",
    "    groupOutput = []\n",
    "\n",
    "    \n",
    "    group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]\n",
    "    group = cv2.threshold(group, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    cv2.imshow(\"Group\", group)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    \n",
    "    charCnts = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    charCnts = imutils.grab_contours(charCnts)\n",
    "    charCnts = contours.sort_contours(charCnts,method=\"left-to-right\")[0]\n",
    "\n",
    "    \n",
    "    (rois, locs) = extract_digits_and_symbols(group, charCnts)\n",
    "\n",
    "    \n",
    "    for roi in rois:\n",
    "        \n",
    "        scores = []\n",
    "        roi = cv2.resize(roi, (36, 36))\n",
    "\n",
    "        \n",
    "        for charName in charNames:\n",
    "            \n",
    "            result = cv2.matchTemplate(roi, chars[charName],\n",
    "                cv2.TM_CCOEFF)\n",
    "            (_, score, _, _) = cv2.minMaxLoc(result)\n",
    "            scores.append(score)\n",
    "\n",
    "      \n",
    "        groupOutput.append(charNames[np.argmax(scores)])\n",
    "\n",
    "    \n",
    "    cv2.rectangle(image, (gX - 10, gY + delta - 10),(gX + gW + 10, gY + gY + delta), (0, 0, 255), 2)\n",
    "    cv2.putText(image, \"\".join(groupOutput),(gX - 10, gY + delta - 25), cv2.FONT_HERSHEY_SIMPLEX,0.95, (0, 0, 255), 3)\n",
    "\n",
    "    \n",
    "    output.append(\"\".join(groupOutput))\n",
    "\n",
    "\n",
    "print(\"Check OCR: {}\".format(\" \".join(output)))\n",
    "cv2.imshow(\"Check OCR\", image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img=cv2.imread('chq1.png')\n",
    "cv2.imshow('chq1',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img=cv2.imread('chq1.png',0)\n",
    "cv2.imshow('image',img)\n",
    "print(img)\n",
    "k=cv2.waitKey(0)\n",
    "if k==27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Quand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import quandl\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import preprocessing,cross_decomposition,svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df=quandl.get('WIKI/GOOGL')\n",
    "#print(df.head())\n",
    "df=df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]\n",
    "df['HL_PCT']=(df['Adj. High']-df['Adj. Close'])/df['Adj. Close']*100.0\n",
    "df['PCT_change']=(df['Adj. Close']-df['Adj. Open'])/df['Adj. Open']*100.0\n",
    "\n",
    "df=df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]\n",
    "\n",
    "\n",
    "\n",
    "forecast_col='Adj. Close'\n",
    "df.fillna(-99999,inplace=True)\n",
    "\n",
    "forecast_out=int(math.ceil(0.01*len(df)))\n",
    "\n",
    "df['label']=df[forecast_col].shift(-forecast_out)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X=np.array(df.drop(['label'],1))\n",
    "\n",
    "y=np.array(df['label'])\n",
    "\n",
    "X=preprocessing.scale(X)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "y=np.array(df['label'])\n",
    "print(len(X),len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris=load_iris()\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(iris.data))\n",
    "print(type(iris.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detection of the signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure, morphology\n",
    "from skimage.color import label2rgb\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "# read the input image\n",
    "img = cv2.imread('chq2.jpg', 0)\n",
    "img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)[1]  # ensure binary\n",
    "#img=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "#img=cv2.Canny(img,125,255)\n",
    "# connected component analysis by scikit-learn framework\n",
    "blobs = img > img.mean()\n",
    "blobs_labels = measure.label(blobs, background=1)\n",
    "image_label_overlay = label2rgb(blobs_labels, image=img)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "the_biggest_component = 0\n",
    "total_area = 0\n",
    "counter = 0\n",
    "average = 0.0\n",
    "for region in regionprops(blobs_labels):\n",
    "    if (region.area > 10):\n",
    "        total_area = total_area + region.area\n",
    "        counter = counter + 1\n",
    "    # print region.area # (for debugging)\n",
    "    # take regions with large enough areas\n",
    "    if (region.area >= 250):\n",
    "        if (region.area > the_biggest_component):\n",
    "            the_biggest_component = region.area\n",
    "\n",
    "average = (total_area/counter)\n",
    "#print(\"the_biggest_component: \" + str(the_biggest_component))\n",
    "#print(\"average: \" + str(average))\n",
    "\n",
    "# experimental-based ratio calculation, modify it for your cases\n",
    "# a4_constant is used as a threshold value to remove connected pixels\n",
    "# are smaller than a4_constant for A4 size scanned documents\n",
    "a4_constant = ((average/84.0)*250.0)+100\n",
    "#print(\"a4_constant: \" + str(a4_constant))\n",
    "\n",
    "# remove the connected pixels are smaller than a4_constant\n",
    "b = morphology.remove_small_objects(blobs_labels, a4_constant)\n",
    "# save the the pre-version which is the image is labelled with colors\n",
    "# as considering connected components\n",
    "plt.imsave('pre_chq.png', b)\n",
    "\n",
    "# read the pre-version\n",
    "img = cv2.imread('pre_chq.png', 0)\n",
    "# ensure binary\n",
    "img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "# save the the result\n",
    "#cv2.imwrite(\"./outputs/output.png\", img)\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "img = cv2.imread('sign.jpg', 0)\n",
    "img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)[1]\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing two signature using SSIM and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skimage.measure import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import measure\n",
    "\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the\n",
    "    # sum of the squared difference between the two images;\n",
    "    # NOTE: the two images must have the same dimension\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    \n",
    "    # return the MSE, the lower the error, the more \"similar\"\n",
    "    # the two images are\n",
    "    return err\n",
    "\n",
    "def compare_images(imageA, imageB, title):\n",
    "    # compute the mean squared error and structural similarity\n",
    "    # index for the images\n",
    "    m = mse(imageA, imageB)\n",
    "    #s = ssim(imageA, imageB)\n",
    "    s = measure.compare_ssim(imageA, imageB)\n",
    "\n",
    "    # setup the figure\n",
    "    fig = plt.figure(title)\n",
    "    plt.suptitle(\"MSE: %.2f, SSIM: %.2f\" % (m, s))\n",
    "\n",
    "    # show first image\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(imageA, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # show the second image\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(imageB, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # show the images\n",
    "    plt.show()\n",
    "\n",
    "# load the images -- the original, the original + contrast,\n",
    "# and the original + photoshop\n",
    "original = cv2.imread(\"C:/Users/naveen.dagar/start learning python/sign.jpg\")\n",
    "contrast = cv2.imread(\"C:/Users/naveen.dagar/start learning python/sr1.jpg\")\n",
    "#shopped = cv2.imread(\"C:/Users/naveen.dagar/start learning python/python-compare-two-images/python-compare-two-images/images/jp_gates_contrast.png\")\n",
    "\n",
    "# convert the images to grayscale\n",
    "original = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
    "contrast = cv2.cvtColor(contrast, cv2.COLOR_BGR2GRAY)\n",
    "#shopped = cv2.cvtColor(shopped, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# initialize the figure\n",
    "fig = plt.figure(\"Images\")\n",
    "images = (\"Original\", original), (\"Contrast\", contrast)\n",
    "\n",
    "# loop over the images\n",
    "for (i, (name, image)) in enumerate(images):\n",
    "# show the image\n",
    "    ax = fig.add_subplot(1, 3, i + 1)\n",
    "    ax.set_title(name)\n",
    "    plt.imshow(image, cmap = plt.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "\n",
    "# compare the images\n",
    "compare_images(original, original, \"Original vs. Original\")\n",
    "compare_images(original, contrast, \"Original vs. Contrast\")\n",
    "#compare_images(original, shopped, \"Original vs. Photoshopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work on 29 nov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure, morphology\n",
    "from skimage.color import label2rgb\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "# read the input image\n",
    "img = cv2.imread('chq2.jpg', 0)\n",
    "img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)[1]  # ensure binary\n",
    "#img=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "#img=cv2.Canny(img,125,255)\n",
    "# connected component analysis by scikit-learn framework\n",
    "blobs = img > img.mean()\n",
    "blobs_labels = measure.label(blobs, background=1)\n",
    "image_label_overlay = label2rgb(blobs_labels, image=img)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "the_biggest_component = 0\n",
    "total_area = 0\n",
    "counter = 0\n",
    "average = 0.0\n",
    "for region in regionprops(blobs_labels):\n",
    "    if (region.area > 10):\n",
    "        total_area = total_area + region.area\n",
    "        counter = counter + 1\n",
    "    # print region.area # (for debugging)\n",
    "    # take regions with large enough areas\n",
    "    if (region.area >= 250):\n",
    "        if (region.area > the_biggest_component):\n",
    "            the_biggest_component = region.area\n",
    "\n",
    "average = (total_area/counter)\n",
    "#print(\"the_biggest_component: \" + str(the_biggest_component))\n",
    "#print(\"average: \" + str(average))\n",
    "\n",
    "# experimental-based ratio calculation, modify it for your cases\n",
    "# a4_constant is used as a threshold value to remove connected pixels\n",
    "# are smaller than a4_constant for A4 size scanned documents\n",
    "a4_constant = ((average/95.0)*250.0)+100\n",
    "#print(\"a4_constant: \" + str(a4_constant))\n",
    "\n",
    "# remove the connected pixels are smaller than a4_constant\n",
    "b = morphology.remove_small_objects(blobs_labels, a4_constant)\n",
    "# save the the pre-version which is the image is labelled with colors\n",
    "# as considering connected components\n",
    "plt.imsave('pre_chq.png', b)\n",
    "\n",
    "# read the pre-version\n",
    "img = cv2.imread('pre_chq.png', 0)\n",
    "# ensure binary\n",
    "#img=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 5);\n",
    "img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "# save the the result\n",
    "#cv2.imwrite(\"./outputs/output.png\", img)\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import editdistance\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from DataLoader import Batch, DataLoader, FilePaths\n",
    "from SamplePreprocessor import preprocessor, wer\n",
    "from Model import DecoderType, Model\n",
    "from SpellChecker import correct_sentence\n",
    "\n",
    "\n",
    "def train(model, loader):\n",
    "    \"\"\" Train the neural network \"\"\"\n",
    "    epoch = 0  # Number of training epochs since start\n",
    "    bestCharErrorRate = float('inf')  # Best valdiation character error rate\n",
    "    noImprovementSince = 0  # Number of epochs no improvement of character error rate occured\n",
    "    earlyStopping = 25  # Stop training after this number of epochs without improvement\n",
    "    batchNum = 0\n",
    "\n",
    "    totalEpoch = loader.trainSamples//Model.batchSize # loader.numTrainSamplesPerEpoch\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch, '/', totalEpoch)\n",
    "\n",
    "        # Train\n",
    "        print('Train neural network')\n",
    "        loader.trainSet()\n",
    "        while loader.hasNext():\n",
    "            batchNum += 1\n",
    "            iterInfo = loader.getIteratorInfo()\n",
    "            batch = loader.getNext()\n",
    "            loss = model.trainBatch(batch, batchNum)\n",
    "            print('Batch:', iterInfo[0], '/', iterInfo[1], 'Loss:', loss)\n",
    "\n",
    "        # Validate\n",
    "        charErrorRate, addressAccuracy, wordErrorRate = validate(model, loader)\n",
    "        cer_summary = tf.Summary(value=[tf.Summary.Value(\n",
    "            tag='charErrorRate', simple_value=charErrorRate)])  # Tensorboard: Track charErrorRate\n",
    "        # Tensorboard: Add cer_summary to writer\n",
    "        model.writer.add_summary(cer_summary, epoch)\n",
    "        address_summary = tf.Summary(value=[tf.Summary.Value(\n",
    "            tag='addressAccuracy', simple_value=addressAccuracy)])  # Tensorboard: Track addressAccuracy\n",
    "        # Tensorboard: Add address_summary to writer\n",
    "        model.writer.add_summary(address_summary, epoch)\n",
    "        wer_summary = tf.Summary(value=[tf.Summary.Value(\n",
    "            tag='wordErrorRate', simple_value=wordErrorRate)])  # Tensorboard: Track wordErrorRate\n",
    "        # Tensorboard: Add wer_summary to writer\n",
    "        model.writer.add_summary(wer_summary, epoch)\n",
    "\n",
    "        # If best validation accuracy so far, save model parameters\n",
    "        if charErrorRate < bestCharErrorRate:\n",
    "            print('Character error rate improved, save model')\n",
    "            bestCharErrorRate = charErrorRate\n",
    "            noImprovementSince = 0\n",
    "            model.save()\n",
    "            open(FilePaths.fnAccuracy, 'w').write(\n",
    "                'Validation character error rate of saved model: %f%%' % (charErrorRate*100.0))\n",
    "        else:\n",
    "            print('Character error rate not improved')\n",
    "            noImprovementSince += 1\n",
    "\n",
    "        # Stop training if no more improvement in the last x epochs\n",
    "        if noImprovementSince >= earlyStopping:\n",
    "            print('No more improvement since %d epochs. Training stopped.' %\n",
    "                  earlyStopping)\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    \"\"\" Validate neural network \"\"\"\n",
    "    print('Validate neural network')\n",
    "    loader.validationSet()\n",
    "    numCharErr = 0\n",
    "    numCharTotal = 0\n",
    "    numWordOK = 0\n",
    "    numWordTotal = 0\n",
    "\n",
    "    totalCER = []\n",
    "    totalWER = []\n",
    "    while loader.hasNext():\n",
    "        iterInfo = loader.getIteratorInfo()\n",
    "        print('Batch:', iterInfo[0], '/', iterInfo[1])\n",
    "        batch = loader.getNext()\n",
    "        recognized = model.inferBatch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n",
    "            numWordTotal += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gtTexts[i])\n",
    "            ## editdistance\n",
    "            currCER = dist/max(len(recognized[i]), len(batch.gtTexts[i]))\n",
    "            totalCER.append(currCER)\n",
    "\n",
    "            currWER = wer(recognized[i].split(), batch.gtTexts[i].split())\n",
    "            totalWER.append(currWER)\n",
    "\n",
    "            numCharErr += dist\n",
    "            numCharTotal += len(batch.gtTexts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' +\n",
    "                  batch.gtTexts[i] + '\"', '->', '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # Print validation result\n",
    "    charErrorRate = sum(totalCER)/len(totalCER)\n",
    "    addressAccuracy = numWordOK / numWordTotal\n",
    "    wordErrorRate = sum(totalWER)/len(totalWER)\n",
    "    print('Character error rate: %f%%. Address accuracy: %f%%. Word error rate: %f%%' %\n",
    "          (charErrorRate*100.0, addressAccuracy*100.0, wordErrorRate*100.0))\n",
    "    return charErrorRate, addressAccuracy, wordErrorRate\n",
    "\n",
    "\n",
    "def load_different_image():\n",
    "    imgs = []\n",
    "    for i in range(1, Model.batchSize):\n",
    "       imgs.append(preprocessor(cv2.imread(\"../data/check_image/a ({}).png\".format(i), cv2.IMREAD_GRAYSCALE), Model.imgSize, enhance=False))\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def generate_random_images():\n",
    "    return np.random.random((Model.batchSize, Model.imgSize[0], Model.imgSize[1]))\n",
    "\n",
    "\n",
    "def infer(model, fnImg):\n",
    "    \"\"\" Recognize text in image provided by file path \"\"\"\n",
    "    img = preprocessor(cv2.imread(fnImg, cv2.IMREAD_GRAYSCALE), imgSize=Model.imgSize)\n",
    "    if img is None:\n",
    "        print(\"Image not found\")\n",
    "\n",
    "    imgs = load_different_image()\n",
    "    imgs = [img] + imgs\n",
    "    batch = Batch(None, imgs)\n",
    "    recognized = model.inferBatch(batch)  # recognize text\n",
    "\n",
    "    print(\"Without Correction\", recognized[0])\n",
    "    print(\"With Correction\", correct_sentence(recognized[0]))\n",
    "    return recognized[0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function \"\"\"\n",
    "    # Opptional command line args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--train\", help=\"train the neural network\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--validate\", help=\"validate the neural network\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--wordbeamsearch\", help=\"use word beam search instead of best path decoding\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    decoderType = DecoderType.BestPath\n",
    "    if args.wordbeamsearch:\n",
    "        decoderType = DecoderType.WordBeamSearch\n",
    "\n",
    "    # Train or validate on Cinnamon dataset\n",
    "    if args.train or args.validate:\n",
    "        # Load training data, create TF model\n",
    "        loader = DataLoader(FilePaths.fnTrain, Model.batchSize,\n",
    "                            Model.imgSize, Model.maxTextLen, load_aug=True)\n",
    "\n",
    "        # Execute training or validation\n",
    "        if args.train:\n",
    "            model = Model(loader.charList, decoderType)\n",
    "            train(model, loader)\n",
    "        elif args.validate:\n",
    "            model = Model(loader.charList, decoderType, mustRestore=False)\n",
    "            validate(model, loader)\n",
    "\n",
    "    # Infer text on test image\n",
    "    else:\n",
    "        print(open(FilePaths.fnAccuracy).read())\n",
    "        model = Model(open(FilePaths.fnCharList).read(),\n",
    "                      decoderType, mustRestore=False)\n",
    "        infer(model, FilePaths.fnInfer)\n",
    "\n",
    "\n",
    "def infer_by_web(path, option):\n",
    "    decoderType = DecoderType.BestPath\n",
    "    print(open(FilePaths.fnAccuracy).read())\n",
    "    model = Model(open(FilePaths.fnCharList).read(),\n",
    "                  decoderType, mustRestore=False)\n",
    "    recognized = infer(model, path)\n",
    "\n",
    "    return recognized\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
